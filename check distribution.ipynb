{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "import baselines.common.tf_util as U\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "class Pd(object):\n",
    "    \"\"\"\n",
    "    A particular probability distribution\n",
    "    \"\"\"\n",
    "    def flatparam(self):\n",
    "        raise NotImplementedError\n",
    "    def mode(self):\n",
    "        raise NotImplementedError\n",
    "    def neglogp(self, x):\n",
    "        # Usually it's easier to define the negative logprob\n",
    "        raise NotImplementedError\n",
    "    def kl(self, other):\n",
    "        raise NotImplementedError\n",
    "    def entropy(self):\n",
    "        raise NotImplementedError\n",
    "    def sample(self):\n",
    "        raise NotImplementedError\n",
    "    def logp(self, x):\n",
    "        return - self.neglogp(x)\n",
    "\n",
    "class CategoricalPd(Pd):\n",
    "    def __init__(self, logits):\n",
    "        self.logits = logits\n",
    "    def flatparam(self):\n",
    "        return self.logits\n",
    "    def mode(self):\n",
    "        return tf.argmax(self.logits, axis=-1)\n",
    "    def neglogp(self, x):\n",
    "        # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n",
    "        # Note: we can't use sparse_softmax_cross_entropy_with_logits because\n",
    "        #       the implementation does not allow second-order derivatives...\n",
    "        one_hot_actions = tf.one_hot(x, self.logits.get_shape().as_list()[-1])\n",
    "        return tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=self.logits,\n",
    "            labels=one_hot_actions)\n",
    "    def kl(self, other):\n",
    "        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n",
    "        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)\n",
    "        ea0 = tf.exp(a0)\n",
    "        ea1 = tf.exp(a1)\n",
    "        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n",
    "        p0 = ea0 / z0\n",
    "        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n",
    "    def entropy(self):\n",
    "        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n",
    "        ea0 = tf.exp(a0)\n",
    "        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n",
    "        p0 = ea0 / z0\n",
    "        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n",
    "    def sample(self):\n",
    "        u = tf.random_uniform(tf.shape(self.logits))\n",
    "        return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)\n",
    "    @classmethod\n",
    "    def fromflat(cls, flat):\n",
    "        return cls(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.12692805], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=225, shape=(1,), dtype=float32, numpy=array([1.1353353], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[-1.0, 1.0]])\n",
    "pd = CategoricalPd(x)\n",
    "#a = pd.sample()\n",
    "a = tf.constant([1])\n",
    "print(pd.neglogp(a))\n",
    "tf.exp(pd.neglogp(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1269]])\n",
      "More decimals -0.1269280910\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Categorical\n",
    "FixedCategorical = torch.distributions.Categorical\n",
    "\n",
    "old_sample = FixedCategorical.sample\n",
    "FixedCategorical.sample = lambda self: old_sample(self).unsqueeze(-1)\n",
    "\n",
    "log_prob_cat = FixedCategorical.log_prob\n",
    "FixedCategorical.log_probs = lambda self, actions: log_prob_cat(\n",
    "    self, actions.squeeze(-1)).view(actions.size(0), -1).sum(-1).unsqueeze(-1)\n",
    "\n",
    "FixedCategorical.mode = lambda self: self.probs.argmax(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "\n",
    "x = torch.Tensor([[-1.0,1.0]])\n",
    "dist = FixedCategorical(logits=x)\n",
    "a = torch.Tensor([1])\n",
    "print(dist.log_probs(a))\n",
    "print('More decimals', '{:.10f}'.format(dist.log_probs(a).item()))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum grads 1.1353353262\n"
     ]
    }
   ],
   "source": [
    "exp = torch.exp(-dist.log_probs(a))\n",
    "print('Sum grads', '{:.10f}'.format(exp.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum grads 1.1353353262\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "x = torch.Tensor([[-1.0,1.0]])\n",
    "a = torch.Tensor([1])\n",
    "criterion1 = nn.CrossEntropyLoss()\n",
    "loss1 = criterion1(x, a.long())\n",
    "print('Sum grads', '{:.10f}'.format(torch.exp(loss1).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1353353"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
