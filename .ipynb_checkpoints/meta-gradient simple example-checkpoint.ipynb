{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inspiration from here https://discuss.pytorch.org/t/backpropagating-through-multiple-optimizer-steps/26017/2\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Imports\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Fix seeds\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = nn.Linear(10, 2)\n",
    "weights = nn.Parameter(torch.ones(10))\n",
    "\n",
    "def get_targets(x):\n",
    "    \"\"\"Random function to learn.\"\"\"\n",
    "    return 10 + x ** 2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "meta_opt = optim.Adam(weights.parameters())\n",
    "opt  = optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "# inner loop optimize the model weights\n",
    "meta_loss = 0\n",
    "\n",
    "\n",
    "for e in range(1):\n",
    "    \n",
    "    # Get data\n",
    "    x = torch.randn(10, 10)\n",
    "    \n",
    "    # Get Targets\n",
    "    y = get_targets(x)\n",
    "    \n",
    "    # Predict targets with model - forward pass\n",
    "    output = model(x)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, target)\n",
    "    loss = torch.sum(loss * weights)\n",
    "    \n",
    "    # Backward pass\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step() # theta' = theta + f(weights)\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    #print(loss)\n",
    "    #print(torch.sum(loss * weights))\n",
    "\n",
    "\n",
    "    \n",
    "    model.zero_grad()\n",
    "\n",
    "    output = model(x) # y = model(theta', x)\n",
    "    meta_loss_ = criterion(output,target)\n",
    "    meta_loss += torch.sum(meta_loss_ * weights_test)\n",
    "        \n",
    "    print(meta_loss)\n",
    "    meta_loss.backward()\n",
    "    print(weights)\n",
    "\n",
    "    meta_opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1648, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1553, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1468, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1391, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1321, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1200, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1147, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1098, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1053, grad_fn=<NllLossBackward>)\n",
      "tensor(0.1011, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0972, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0936, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0903, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0872, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0843, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0815, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0789, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0765, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0742, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0721, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0700, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0681, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0663, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0645, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0629, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0613, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0598, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0584, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0570, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0557, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0545, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0533, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0521, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0511, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0500, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0490, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0480, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0471, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0462, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0453, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0445, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0437, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0429, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0421, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0414, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0407, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0400, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0394, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0388, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0381, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0375, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0370, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0364, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0359, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0353, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0348, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0343, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0338, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0333, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0329, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0324, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0320, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0316, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0312, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0308, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0304, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0300, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0296, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0293, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0289, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0286, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0282, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0279, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0276, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0272, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0269, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0266, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0263, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0260, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0258, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0255, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0252, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0249, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0247, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0244, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0242, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0239, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0237, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0235, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0232, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0230, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0228, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0226, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0224, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0221, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0219, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0217, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0215, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0213, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(10, 2)\n",
    "temp_weights=[w.clone() for w in list(model.parameters())]\n",
    "\n",
    "weights = torch.ones(10, requires_grad=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "meta_opt = optim.Adam(model.parameters())\n",
    "\n",
    "target = torch.empty(1, dtype=torch.long).random_(2)\n",
    "\n",
    "alpha = 0.001\n",
    "\n",
    "x = torch.randn(1, 10)\n",
    "\n",
    "for e in range(100):\n",
    "    # inner loop optimize the model weights\n",
    "    meta_loss = 0\n",
    "    \n",
    "    for _ in range(10):\n",
    "        \n",
    "        output = F.linear(x,temp_weights[0],temp_weights[1])\n",
    "        loss = criterion(output, target)\n",
    "        w_loss = (loss * weights).sum() / weights.sum()\n",
    "\n",
    "        grads = torch.autograd.grad(w_loss,temp_weights)\n",
    "        temp_weights = [w-alpha*g for w,g in zip(temp_weights,grads)] #temporary update of weights\n",
    "        output = F.linear(x,temp_weights[0],temp_weights[1])\n",
    "        meta_loss += criterion(output,target)\n",
    "        #meta_loss = (loss * weights).sum() / weights.sum()\n",
    "    print(criterion(output,target))\n",
    "    \n",
    "    metagrads=torch.autograd.grad(meta_loss, model.parameters())\n",
    "    for w,g in zip(model.parameters(),metagrads):\n",
    "        w.grad=g\n",
    "        \n",
    "    meta_opt.step()\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
